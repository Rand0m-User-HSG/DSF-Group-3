##Maboost##

--------------##Introduction##-------------

# In this script we perform a logistic regression for a classication task
# The goal is to predict the degree of severity of an accident
# There are 3 dgrees of severity: light injuries, severe injuries, fatalities
# We perdorm a multiclass classificaiton with help of boosting

rm(list =ls())

#maboost package is the extension of adaboost package for multiclass classification
#install.packages("maboost")
#install.packages("caret")
library(rpart)
library(C50)
library(maboost)
library(lattice)
library(tidyverse)
library(caret)

load("./Data/covariate_matrix.RData")
load("./Data/Y_vector_classification.RData")

--------------##Boosted multicass classification##-------------

num_degrees = 3
n = dim(X_matrix)[1]
p = dim(X_matrix)[2]
pred = matrix(NA, nrow = nrow(X_matrix), ncol = 1)
y_classified = rep(NA, length(Y_vector))
X_matrix = data.frame(X_matrix)

model_maboost = maboost(X_matrix, Y_vector, iter = 5, nu = .1, C50tree = T, C5.0Control(CF = .2, minCase = 128))

pred = predict(model_maboost,X_matrix,type="class")
  
y_classified =  pred

Empirical_error = length(which(y_classified != Y_vector)) / n

# Empirical_error = 0.2142199


--------------##10-folds cross-validation##-------------

k = 10
num_degrees = 3
Empirical_error_cv = rep(NA, k)
pred = matrix(NA, nrow = nrow(X_matrix)/k, ncol = k)
y_classified_cv = rep(NA, length(Y_vector))
X_matrix = data.frame(X_matrix)

for (i in 1:k) {
  
  yn = Y_vector[-((1+(i-1)*nrow(X_matrix)/k):(i*nrow(X_matrix)/k))]
  xn = X_matrix[-((1+(i-1)*nrow(X_matrix)/k):(i*nrow(X_matrix)/k)),]
  xn_test = data.frame(X_matrix[(1+(i-1)*nrow(X_matrix)/k):(i*nrow(X_matrix)/k),])
  yn_test = Y_vector[(1+(i-1)*nrow(X_matrix)/k):(i*nrow(X_matrix)/k)]
  
  model_maboost = maboost(x = xn, y = yn, iter = 5, nu = .1, C50tree = T, C5.0Control(CF = .2, minCase = 128))

  pred = predict(model_maboost,xn_test,type="class");
  
  y_classified_cv[(1+(i-1)*nrow(X_matrix)/k):(i*nrow(X_matrix)/k)] <- pred
  
  Empirical_error_cv = length(which(y_classified_cv != Y_vector)) / n
}

# Empirical_error_cv = 0.2142199

--------------##Misclassification matrix##-------------
#The code is based on the script of JPO, Day 4Exercise4_Handwriting_recognition.R 

misclassification_matrix = matrix(NA, num_degrees, num_degrees)

for (i in 1:num_degrees) {
  for (j in 1:num_degrees) {
    misclassification_matrix[i, j] = length(which((yn == i) & (y_classified == j))) / length(which((yn == i)))
  }
}

--------------##Unsuccessful attempts##-------------
#The code is based on the script of JPO, Day 4 Exercise2_ClassifierAnemia_2features.R 

#attempt on using optim function 
xn_test = as.matrix(xn_test)
Empirical_error_function = function(y_classified) {
  length(which( cbind(rep(1,5429), xn) %*% y_classified < 1)) +
    length(which(cbind(rep(1,5429), xn)  %*% y_classified >=2))
}

optim_result = optim(par=rep(0,3), Empirical_error_function , method="SANN") # we can not print itertions with optim()
result = optim_result$par

#not working but should look something like that 
